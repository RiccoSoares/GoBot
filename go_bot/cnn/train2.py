import torch
import numpy as np
import os

from torch.nn import CrossEntropyLoss
from torch.optim import SGD
from torch.utils.data import DataLoader
from model import nineNet
from dataset import BoardsDataset

#using data provided from https://github.com/maxpumperla/deep_learning_and_the_game_of_go/tree/chapter_6/code/generated_games
#the dataset contains 11705 board positions and it's answers, generated by pure mcts algorithm.
features_path = './data/mcts_generated/features-200.npy'
labels_path = './data/mcts_generated/labels-200.npy'

np_features = np.load(features_path)
np_labels = np.load(labels_path)

#dividing all data into train and validation sets while casting it to pytorch tensors.
np_train_features = np_features[:int(0.8*len(np_features))]
np_train_labels = np_labels[:int(0.8*len(np_labels))]

np_validation_features = np_features[int(0.8*len(np_features)):]
np_validation_labels = np_labels[int(0.8*len(np_labels)):]

#instantiating the dataset and the dataloader
trainset = BoardsDataset(np_train_features, np_train_labels)
testset = BoardsDataset(np_validation_features, np_validation_labels, mode='test')

trainloader = DataLoader(trainset, batch_size=1, shuffle=True)
testloader = DataLoader(testset, batch_size=4, shuffle=False) #there's no need to shuffle the test loader.

#instantiating the network
net = nineNet()

#defining the loss and the optimizer
criterion = CrossEntropyLoss()
learning_rate = 1e-3 #initial learning rate for the optimizer
optimizer = SGD(net.parameters(), lr=learning_rate)

#the training loop itself.
n_epochs = 100
for epoch in range(n_epochs):
	for minibatch in trainloader:
		#training loop	
		boards = minibatch["board"]
		labels = minibatch["move"]
		
		model_output = net(boards)
		loss = criterion(model_output, labels)
		
		optimizer.zero_grad()
		loss.backward()
		optimizer.step()
	
	print("Epoch: %d, Loss: %f" % (epoch, float(loss))) #printing the loss of a random minibatch
	



