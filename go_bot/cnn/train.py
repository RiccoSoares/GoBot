import torch
import numpy as np
import os

from torch.nn import CrossEntropyLoss, NLLLoss
from torch.optim import SGD
from torch.utils.data import DataLoader
from model import nineNet, nineNetv2
from dataset import BoardsDataset

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #to support the use of gpu.

#using data provided from https://github.com/maxpumperla/deep_learning_and_the_game_of_go/tree/chapter_6/code/generated_games
#the dataset contains 11705 board positions and it's answers, generated by pure mcts algorithm.
features_path = './data/mcts_generated/features-200.npy'
labels_path = './data/mcts_generated/labels-200.npy'

np_features = np.load(features_path)
np_labels = np.load(labels_path)

#dividing all data into train and validation sets while casting it to pytorch tensors.
np_train_features = np_features[:int(0.8*len(np_features))]
np_train_labels = np_labels[:int(0.8*len(np_labels))]

np_validation_features = np_features[int(0.8*len(np_features)):]
np_validation_labels = np_labels[int(0.8*len(np_labels)):]

#instantiating the dataset and the dataloader
trainset = BoardsDataset(np_train_features, np_train_labels)
validationset = BoardsDataset(np_validation_features, np_validation_labels)

trainloader = DataLoader(trainset, batch_size=7, shuffle=True)
val_loader = DataLoader(validationset, batch_size=7, shuffle=False) #there's no need to shuffle the test loader.

#instantiating the network
net = nineNetv2()
net.to(device) #pass the model to gpu if possible

#defining the loss and the optimizer
criterion = CrossEntropyLoss()

learning_rate = 1e-2 #initial learning rate for the optimizer
optimizer = SGD(net.parameters(), lr=learning_rate)

#the training loop itself.
n_epochs = 200
val_scores = []
for epoch in range(n_epochs):
	training_loss = 0
	for minibatch in trainloader:
		#training loop	
		boards = minibatch["board"].to(device)
		labels = minibatch["move"].to(device)
		
		model_output = net(boards)
		loss = criterion(model_output, labels)
		
		optimizer.zero_grad()
		loss.backward()
		optimizer.step() #the model's parameters are updated each minibatch
		training_loss += loss
		
	val_inputs = 0
	correct_predicts = 0
	for val_minibatch in val_loader:
		val_boards = val_minibatch["board"].to(device)
		val_labels = val_minibatch["move"].to(device)
		
		val_output = net(val_boards)
		predicts = torch.argmax(val_output, dim=1)
		
		val_inputs += val_labels.shape[0]
		correct_predicts += int((predicts == val_labels).sum())
		
	val_acc = correct_predicts / val_inputs
	val_scores.append(val_acc)
	print('')
	print("Epoch: %d, Loss: %f" % (epoch, float(training_loss/len(trainloader)))) #printing the average training loss per epoch
	print("Validation Accuracy: ", str(100*val_acc) + '%')
	print('')
	
print('Training finished.')
print('Highest validation accuracy achieved: ', max(val_scores))
print('In epoch: ', val_scores.index(max(val_scores)))



